{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cliff Walking with Q-Learning (Reinforcement Learning)\n",
    "\n",
    "This notebook demonstrates how an agent can learn to navigate a grid-like environment (CliffWalking) using Q-learning.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "To run this notebook, you need:\n",
    "\n",
    "- **Numpy**: Version `1.26.4`  \n",
    "- **gym**: Version `0.26.2` \n",
    "- **matplotlib**: For plotting learning performance\n",
    "\n",
    "You can check your installed versions by running:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Gym version:\", gym.__version__)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "If not installed, you can do so via:\n",
    "```python\n",
    "pip install numpy matplotlib gym\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Or inside Jupyter/VS Code:\n",
    "\n",
    " ```python\n",
    " !pip install numpy matplotlib gym\n",
    "# OR\n",
    "%pip install numpy matplotlib gym\n",
    "```\n",
    "\n",
    "\n",
    "### Notes: \n",
    "- It's recommended to use a virtual environment to isolate your dependencies for this project.\n",
    "\n",
    "    ```python\n",
    "    ## Creating a vertual environment\n",
    "        python -m venv your_env_name\n",
    "\n",
    "        #### Activate it (Windows)\n",
    "        ./your_env_name/Scripts/activate\n",
    "        # Activate it (Mac/Linux)\n",
    "        source your_env_name/bin/activate\n",
    "    ```\n",
    "- Run it in a Juypter notebook or google colab for better visualization of how the model is learning (jupyter notebook is highly recommended).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 721.7/721.7 kB 9.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (2.2.1)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827739 sha256=29eb5b4ff2c2b98246c0188c5de1fdcb085b34ed84b34049abe35b1f1492c466\n",
      "  Stored in directory: c:\\users\\kiit\\appdata\\local\\pip\\cache\\wheels\\1c\\77\\9e\\9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, gym\n",
      "Successfully installed gym-0.26.2 gym_notices-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Episode 5, Step 13\n",
      "Action: DOWN, Reward: -1\n",
      "\n",
      "Environment Grid:\n",
      "0 0 0 0 0 0 0 0 0 0 0 0\n",
      "0 0 0 0 0 0 0 0 0 0 0 0\n",
      "· · · · · · · · · · · ·\n",
      "S * * * * * * * * * * A\n",
      "\n",
      "Q-values for current state ((3, 11)):\n",
      "UP: 0.0000\n",
      "RIGHT: 0.0000\n",
      "DOWN: 0.0000\n",
      "LEFT: 0.0000\n",
      "\n",
      "\n",
      "Evaluation Episode 5 finished.\n",
      "Total Reward: -13, Path Length: 14\n",
      "Path matches a training optimal path: Yes\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "# np.random.seed(42)\n",
    "\n",
    "# We are creating the Cliff Walking environment\n",
    "env = gym.make(\"CliffWalking-v0\")\n",
    "\n",
    "# Value initialization\n",
    "num_states = env.observation_space.n  # Number of states in the environment\n",
    "num_actions = env.action_space.n  # Number of possible actions in the environment\n",
    "print(\"Number of states:\", num_states)\n",
    "print(\"Number of actions:\", num_actions)\n",
    "\n",
    "# Here we then define the action meanings for better visualization\n",
    "action_meanings = {0: \"UP\", 1: \"RIGHT\", 2: \"DOWN\", 3: \"LEFT\"}\n",
    "\n",
    "# We then create a helper function to convert state (single integer) to row, col coordinates\n",
    "def state_to_coords(state):\n",
    "    # considering the CliffWalking grid which is 4x12\n",
    "    nrow, ncol = 4, 12\n",
    "    row = state // ncol\n",
    "    col = state % ncol\n",
    "    return row, col\n",
    "\n",
    "\n",
    "# *************We create a function to visualize the cliff walking environment*********\n",
    "def print_cliffwalking_grid(agent_position, path_history=None, q_values_for_current_state=None):\n",
    "    nrow, ncol = 4, 12\n",
    "    grid = np.full((nrow, ncol), '0', dtype=str)\n",
    "\n",
    "    # Mark the cliff (Danger zone)\n",
    "    grid[3, 1:11] = '*'\n",
    "\n",
    "    # Mark start (S), and the goal (G)\n",
    "    grid[3, 0] = 'S'\n",
    "    grid[3, 11] = 'G'\n",
    "\n",
    "    # Mark path history if provided (enhancement, good for understanding)\n",
    "    if path_history:\n",
    "        for pos_state in path_history:\n",
    "            row, col = state_to_coords(pos_state)\n",
    "            # Avoid overwriting S, G, or current A with path marker\n",
    "            if grid[row, col] not in ['S', 'G']:\n",
    "                grid[row, col] = '·'\n",
    "\n",
    "    # Mark agent position which is an agent_position and is integer state\n",
    "    agent_row, agent_col = state_to_coords(agent_position)\n",
    "    grid[agent_row, agent_col] = 'A'\n",
    "\n",
    "    # Displaying the grid\n",
    "    print(\"\\nEnvironment Grid:\")\n",
    "    for r in range(nrow):\n",
    "        print(' '.join(grid[r]))\n",
    "\n",
    "    # Displaying Q-values for current state\n",
    "    if q_values_for_current_state is not None:\n",
    "        print(\"\\nQ-values for current state (\" +\n",
    "              str(state_to_coords(agent_position)) + \"):\")\n",
    "        for action_idx in range(num_actions):\n",
    "            print(\n",
    "                f\"{action_meanings[action_idx]}: {q_values_for_current_state[action_idx]:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "# ***********We then create a function to plot the learning curve ***************\n",
    "def plot_learning_curve(rewards_history_list, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(rewards_history_list)\n",
    "    plt.title(f\"{title} Learning Curve\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    window_size = min(50, len(rewards_history_list) //\n",
    "                      10 if len(rewards_history_list) > 0 else 0)\n",
    "    if window_size > 0:\n",
    "        rolling_mean = np.convolve(rewards_history_list, np.ones(\n",
    "            window_size)/window_size, mode='valid')\n",
    "        plt.plot(np.arange(window_size-1, len(rewards_history_list)), rolling_mean, 'r-',  # Use np.arange for x-axis\n",
    "                 linewidth=2, label=f'Rolling Average ({window_size} episodes)')\n",
    "        plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ****** We then create a function to log training information - better for understanding what is going on **********\n",
    "def log_training_info(episode, current_reward, current_state, chosen_action, next_obs, q_table_for_state, current_epsilon):\n",
    "    print(f\"\\nEpisode {episode + 1}:\")\n",
    "    print(f\"State: {current_state} (Coords: {state_to_coords(current_state)})\")\n",
    "    print(f\"Action: {chosen_action} ({action_meanings[chosen_action]})\")\n",
    "    print(f\"Next State: {next_obs} (Coords: {state_to_coords(next_obs)})\")\n",
    "    print(f\"Reward: {current_reward}\")\n",
    "    print(f\"Epsilon: {current_epsilon:.4f}\")\n",
    "    if current_state < q_table_for_state.shape[0]:\n",
    "        print(\"\\nQ-values for current state:\")\n",
    "        for a_idx in range(num_actions):\n",
    "            print(\n",
    "                f\"{action_meanings[a_idx]}: {q_table_for_state[current_state][a_idx]:.4f}\")\n",
    "\n",
    "\n",
    "# ******We then create a Q-learning algorithm for our project *********\n",
    "def q_learning(episodes=5000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    rewards_history = []\n",
    "    optimal_paths_found = []  # This one is used to track shortest paths found\n",
    "    episode_lengths = []\n",
    "\n",
    "    for episode_idx in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_episode_reward = 0\n",
    "        current_path = [state]\n",
    "        num_steps = 0\n",
    "\n",
    "        # Here we log the periodic detailed training - for seeing what is going on and the path the agent is taking\n",
    "        log_detailed_this_episode = (episode_idx + 1) % 100 == 0\n",
    "\n",
    "        while not done:\n",
    "            num_steps += 1\n",
    "\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()  # This is the agent exploring the environment\n",
    "            else:\n",
    "                action = np.argmax(Q[state])      # This is the agent exploiting the environment\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            total_episode_reward += reward\n",
    "            current_path.append(next_state)\n",
    "\n",
    "            # Q-value of best action in next state\n",
    "            best_next_action_q_value = np.max(Q[next_state])\n",
    "            Q[state, action] = Q[state, action] + alpha * \\\n",
    "                (reward + gamma * best_next_action_q_value - Q[state, action])\n",
    "\n",
    "            if log_detailed_this_episode and num_steps <= 5:  # Log first few steps of milestone episodes\n",
    "                print(\n",
    "                    f\"--- Training: Episode {episode_idx + 1}, Step {num_steps} ---\")\n",
    "                log_training_info(episode_idx, reward, state,\n",
    "                                  action, next_state, Q, epsilon)\n",
    "                print_cliffwalking_grid(\n",
    "                    next_state, current_path, Q[next_state])\n",
    "                if 'ipykernel' in __import__('sys').modules:\n",
    "                    time.sleep(0.1)  # Shorter sleep for training log\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        rewards_history.append(total_episode_reward)\n",
    "        episode_lengths.append(num_steps)\n",
    "\n",
    "        # We then track the optimal paths - shortest ones found so far\n",
    "        if not optimal_paths_found or len(current_path) < len(optimal_paths_found[0]):\n",
    "            optimal_paths_found = [current_path]\n",
    "        elif len(current_path) == len(optimal_paths_found[0]) and current_path not in optimal_paths_found:\n",
    "            optimal_paths_found.append(current_path)\n",
    "\n",
    "        if (episode_idx + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"\\nEpisode: {episode_idx+1}/{episodes}, Steps: {num_steps}, \"\n",
    "                f\"Reward: {total_episode_reward:.2f}, Epsilon: {epsilon:.4f}\")\n",
    "            if optimal_paths_found:\n",
    "                print(\n",
    "                    f\"Found {len(optimal_paths_found)} optimal paths of length {len(optimal_paths_found[0])}\")\n",
    "            # Print final state of milestone training episodes\n",
    "            print_cliffwalking_grid(state, current_path, Q[state])\n",
    "\n",
    "    plot_learning_curve(rewards_history, \"Q-learning Rewards\")\n",
    "    # Plotting episode lengths\n",
    "    plot_learning_curve(episode_lengths, \"Episode Lengths Over Time\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nTraining complete. Found {len(optimal_paths_found)} optimal paths.\")\n",
    "    return Q, optimal_paths_found\n",
    "\n",
    "# *************We then evaluate the trained agent***************\n",
    "def evaluate_policy(q_table, optimal_paths_from_training, n_eval_episodes=10):\n",
    "    print(\"\\n--- Evaluating learned policy ---\")\n",
    "\n",
    "    if optimal_paths_from_training:\n",
    "        print(\n",
    "            f\"Found {len(optimal_paths_from_training)} optimal paths during training (showing up to 3):\")\n",
    "        for i, path_val in enumerate(optimal_paths_from_training[:3]):\n",
    "            print(f\"\\nOptimal Path #{i+1} (Length: {len(path_val)}):\")\n",
    "            state_sequence_str = [str(state_to_coords(s)) for s in path_val]\n",
    "            print(\" -> \".join(state_sequence_str))\n",
    "    else:\n",
    "        print(\"No optimal paths were recorded during training.\")\n",
    "\n",
    "    for episode_num in range(n_eval_episodes):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_eval_reward = 0\n",
    "        eval_steps = 0\n",
    "        eval_path = [state]\n",
    "\n",
    "        print(\n",
    "            f\"\\n--- Evaluation Episode {episode_num + 1}/{n_eval_episodes} ---\")\n",
    "        \n",
    "        # Show initial state with Q-values\n",
    "        print_cliffwalking_grid(state, None, q_table[state])\n",
    "        if 'ipykernel' in __import__('sys').modules:\n",
    "            time.sleep(1)\n",
    "\n",
    "        while not done:\n",
    "            eval_steps += 1\n",
    "            action_to_take = np.argmax(q_table[state])  # Greedy action to decide which direction to take next\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(\n",
    "                action_to_take)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            total_eval_reward += reward\n",
    "            eval_path.append(next_state)\n",
    "            state = next_state\n",
    "\n",
    "            if 'ipykernel' in __import__('sys').modules:\n",
    "                clear_output(wait=True)\n",
    "            else:\n",
    "                print(\"\\033c\", end=\"\")  # ANSI clear screen\n",
    "\n",
    "            print(f\"Evaluation Episode {episode_num + 1}, Step {eval_steps}\")\n",
    "            print(\n",
    "                f\"Action: {action_meanings[action_to_take]}, Reward: {reward}\")\n",
    "            # We then print the current state of the grid using print_cliffwalking_grid.\n",
    "            print_cliffwalking_grid(state, eval_path, q_table[state])\n",
    "            if 'ipykernel' in __import__('sys').modules:\n",
    "                time.sleep(0.5)\n",
    "\n",
    "        print(f\"Evaluation Episode {episode_num + 1} finished.\")\n",
    "        print(\n",
    "            f\"Total Reward: {total_eval_reward}, Path Length: {len(eval_path)}\")\n",
    "        is_optimal_this_run = False\n",
    "        if optimal_paths_from_training:\n",
    "            for opt_path in optimal_paths_from_training:\n",
    "                if eval_path == opt_path:\n",
    "                    is_optimal_this_run = True\n",
    "                    break\n",
    "        print(\n",
    "            f\"Path matches a training optimal path: {'Yes' if is_optimal_this_run else 'No'}\")\n",
    "\n",
    "    env.close() \n",
    "\n",
    "\n",
    "# *************Run the training and evaluation**********\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Q-learning agent training...\")\n",
    "    \n",
    "    learned_Q_table, found_optimal_paths = q_learning(episodes=5000,\n",
    "                                                      alpha=0.1,\n",
    "                                                      gamma=0.9,\n",
    "                                                      epsilon=0.1)\n",
    "\n",
    "    evaluate_policy(learned_Q_table, found_optimal_paths, n_eval_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.2'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
